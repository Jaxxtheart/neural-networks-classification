{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer**\n",
    "\n",
    "Used Youtube channel 'Sentdex' to accomplish this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2305)\n",
    "\n",
    "# Input \"layer\"\n",
    "class Layer_Input:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Initialize weights and biases\n",
    "        \n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "    # Retrieve layer parameters\n",
    "    def get_parameters(self):\n",
    "        pass\n",
    "\n",
    "    # Set weights and biases in a layer instance\n",
    "    def set_parameters(self, weights, biases):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # letâ€™s make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "\n",
    "class Activation_Softmax:\n",
    "\n",
    "# Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "        keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "        keepdims=True)\n",
    "        self.output = probabilities\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * \\\n",
    "                             layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * \\\n",
    "                           layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common accuracy class\n",
    "class Accuracy:\n",
    "\n",
    "    # Calculates an accuracy given predictions and ground truth values\n",
    "    def calculate(self, predictions, y):\n",
    "\n",
    "        # Get comparison results\n",
    "        \n",
    "\n",
    "        # Calculate an accuracy\n",
    "        \n",
    "\n",
    "        # Add accumulated sum of matching values and sample count\n",
    "        \n",
    "\n",
    "        # Return accuracy\n",
    "        pass\n",
    "\n",
    "    # Calculates accumulated accuracy\n",
    "    def calculate_accumulated(self):\n",
    "\n",
    "        # Calculate an accuracy\n",
    "        \n",
    "\n",
    "        # Return the data and regularization losses\n",
    "        pass\n",
    "\n",
    "    # Reset variables for accumulated accuracy\n",
    "    def new_pass(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy_Categorical(Accuracy):\n",
    "\n",
    "    def __init__(self, *, binary=False):\n",
    "        # Binary mode?\n",
    "        pass\n",
    "\n",
    "    # No initialization is needed\n",
    "    def init(self, y):\n",
    "        pass\n",
    "\n",
    "    # Compares predictions to the ground truth values\n",
    "    def compare(self, predictions, y):\n",
    "        # check if in binary mode\n",
    "        if not self.binary and len(y.shape) == 2:\n",
    "            # compare with maximum\n",
    "            pass\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "    # Calculates accumulated loss\n",
    "    def calculate_accumulated(self):\n",
    "        # Calculate mean loss\n",
    "        \n",
    "        # Return the data loss\n",
    "        pass\n",
    "\n",
    "    # Reset variables for accumulated loss\n",
    "    def new_pass(self):\n",
    "        pass\n",
    "\n",
    "    # Set/remember trainable layers\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded, turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model class\n",
    "# class Model:\n",
    "\n",
    "#     def __init__(self):\n",
    "#         # Create a list of network objects\n",
    "        \n",
    "#         # Softmax classifier's output object\n",
    "#         pass\n",
    "\n",
    "#     # Add objects to the model\n",
    "#     def add(self, layer):\n",
    "#         pass\n",
    "\n",
    "#     # Set loss, optimizer and accuracy\n",
    "#     def set(self, *, loss=None, optimizer=None, accuracy=None):\n",
    "#         if loss is not None:\n",
    "#             pass\n",
    "#         if optimizer is not None:\n",
    "#             pass\n",
    "#         if accuracy is not None:\n",
    "#             pass\n",
    "\n",
    "#     # Finalize the model\n",
    "#     def finalize(self):\n",
    "#         # Create and set the input layer\n",
    "#         self.input_layer = Layer_Input()\n",
    "\n",
    "#         # Count all the objects\n",
    "#         layer_count = len(self.layers)\n",
    "\n",
    "#         # Initialize a list containing trainable layers:\n",
    "#         self.trainable_layers = []\n",
    "\n",
    "#         # Iterate the objects\n",
    "#         for i in range(layer_count):\n",
    "\n",
    "#             # If it's the first layer, the previous layer object is the input layer\n",
    "#             if i == 0:\n",
    "#                 self.layers[i].prev = self.input_layer\n",
    "#                 self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "#             # All layers except for the first and the last\n",
    "#             elif i < layer_count - 1:\n",
    "#                 self.layers[i].prev = self.layers[i-1]\n",
    "#                 self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "#             # The last layer - the next object is the loss\n",
    "#             # Also let's save aside the reference to the last object whose output is the model's output\n",
    "#             else:\n",
    "#                 self.layers[i].prev = self.layers[i-1]\n",
    "#                 self.layers[i].next = self.loss\n",
    "#                 self.output_layer_activation = self.layers[i]\n",
    "\n",
    "#             # If layer contains an attribute called \"weights\", it's a trainable layer - add it to the list of trainable layers\n",
    "#             # We don't need to check for biases - checking for weights is enough\n",
    "#             if hasattr(self.layers[i], 'weights'):\n",
    "#                 self.trainable_layers.append(self.layers[i])\n",
    "\n",
    "#         # Update loss object with trainable layers\n",
    "#         if self.loss is not None:\n",
    "#             self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "\n",
    "#         # If output activation is Softmax and loss function is Categorical Cross-Entropy\n",
    "#         # create an object of combined activation and loss function containing faster gradient calculation\n",
    "#         if isinstance(self.layers[-1], Activation_Softmax) and isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
    "#             # Create an object of combined activation and loss functions\n",
    "#             self.softmax_classifier_output = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "#     # Performs forward pass\n",
    "#     def forward(self, X, training):\n",
    "\n",
    "#         # Call forward method on the input layer this will set the output property that the first layer in \"prev\" object is expecting\n",
    "        \n",
    "\n",
    "#         # Call forward method of every object in a chain, pass output of the previous object as a parameter\n",
    "#         for layer in self.layers:\n",
    "#             pass\n",
    "\n",
    "#         # \"layer\" is now the last object from the list, return its output\n",
    "#         return layer.output\n",
    "\n",
    "#     # Train the model\n",
    "#     def train(self, X, y, *, epochs=1, batch_size=None, print_every=1, validation_data=None):\n",
    "\n",
    "#         # Initialize accuracy object\n",
    "        \n",
    "\n",
    "#         # Default value if batch size is not being set\n",
    "        \n",
    "\n",
    "#         # If there is validation data passed, set default number of steps for validation as well\n",
    "#         if validation_data is not None:\n",
    "            \n",
    "#             # For better readability split validation data into X and y values\n",
    "#             pass\n",
    "\n",
    "#         # Calculate number of steps\n",
    "#         if batch_size is not None:\n",
    "#             # calculate the number of training steps according to data and batch size\n",
    "            \n",
    "\n",
    "#             # Dividing rounds down. If there are some remaining data, but not a full batch, this won't include it\n",
    "#             # Add `1` to include this not full batch\n",
    "#             if train_steps * batch_size < len(X):\n",
    "#                 pass\n",
    "            \n",
    "#             # if there is validation data split it like the training data into batches\n",
    "#             if validation_data is not None:\n",
    "                \n",
    "\n",
    "#                 # Dividing rounds down. If there are some remaining data, but nor full batch, this won't include it\n",
    "#                 # Add `1` to include this not full batch\n",
    "#                 if validation_steps * batch_size < len(X_val):\n",
    "#                     pass\n",
    "\n",
    "#         # Main training loop\n",
    "#         for epoch in range(1, epochs+1):\n",
    "\n",
    "#             # Print epoch number\n",
    "            \n",
    "\n",
    "#             # Reset accumulated values in loss and accuracy objects\n",
    "            \n",
    "\n",
    "#             # Iterate over steps\n",
    "#             for step in range(train_steps):\n",
    "#                 # If batch size is not set - train using one step and full dataset\n",
    "#                 if batch_size is None:\n",
    "#                     pass\n",
    "\n",
    "#                 # Otherwise slice a batch\n",
    "#                 else:\n",
    "#                     pass\n",
    "\n",
    "#                 # Perform the forward pass\n",
    "                \n",
    "\n",
    "#                 # Calculate loss\n",
    "                \n",
    "\n",
    "#                 # Get predictions and calculate an accuracy\n",
    "                \n",
    "\n",
    "#                 # Perform backward pass\n",
    "                \n",
    "\n",
    "#                 # Optimize (update parameters)\n",
    "#                 # first call pre-update\n",
    "#                 self.optimizer.pre_update_params()\n",
    "#                 # now update params for each layer\n",
    "#                 for layer in self.trainable_layers:\n",
    "#                     pass\n",
    "#                 # now call post-update params\n",
    "                \n",
    "\n",
    "#                 # Print a summary\n",
    "#                 if not step % print_every or step == train_steps - 1:\n",
    "#                     pass\n",
    "\n",
    "#             # Get and print epoch loss and accuracy\n",
    "#             pass\n",
    "\n",
    "#             # If there is the validation data\n",
    "#             if validation_data is not None:\n",
    "\n",
    "#                 # Evaluate the model:\n",
    "#                 pass\n",
    "\n",
    "#     # Performs backward pass\n",
    "#     def backward(self, output, y):\n",
    "\n",
    "#         # If softmax classifier\n",
    "#         if self.softmax_classifier_output is not None:\n",
    "\n",
    "#             # First call backward method on the combined activation/loss\n",
    "#             # this will set dinputs property\n",
    "            \n",
    "\n",
    "#             # Since we'll not call backward method of the last layer which is Softmax activation\n",
    "#             # as we used combined activation/loss object, let's set dinputs in this object\n",
    "            \n",
    "\n",
    "#             # Call backward method going through all the objects but last\n",
    "#             # in reversed order passing dinputs as a parameter\n",
    "#             for layer in reversed(self.layers[:-1]):\n",
    "#                 pass\n",
    "#             return\n",
    "\n",
    "#         # First call backward method on the loss this will set dinputs property that the last\n",
    "#         # layer will try to access shortly\n",
    "#         pass\n",
    "\n",
    "#         # Call backward method going through all the objects\n",
    "#         # in reversed order passing dinputs as a parameter\n",
    "#         for layer in reversed(self.layers):\n",
    "#             pass\n",
    "\n",
    "#     # Evaluates the model using passed-in dataset\n",
    "#     def evaluate(self, X_val, y_val, *, batch_size=None):\n",
    "\n",
    "#         # Default value if batch size is not being set\n",
    "        \n",
    "\n",
    "#         # Calculate number of steps\n",
    "#         if batch_size is not None:\n",
    "            \n",
    "#             # calculate validation steps\n",
    "            \n",
    "\n",
    "#             # Dividing rounds down. If there are some remaining data, but not a full batch, this won't include it\n",
    "#             # Add `1` to include this not full batch\n",
    "#             if validation_steps * batch_size < len(X_val):\n",
    "#                 pass\n",
    "\n",
    "#         # Reset accumulated values in loss and accuracy objects (new_pass)\n",
    "#         pass\n",
    "\n",
    "#         # Iterate over steps\n",
    "#         for step in range(validation_steps):\n",
    "\n",
    "#             # If batch size is not set - train using one step and full dataset\n",
    "#             if batch_size is None:\n",
    "#                 pass\n",
    "\n",
    "#             # Otherwise slice a batch\n",
    "#             else:\n",
    "#                 pass\n",
    "\n",
    "#             # Perform the forward pass\n",
    "            \n",
    "\n",
    "#             # Calculate the loss\n",
    "            \n",
    "\n",
    "#             # Get predictions and calculate an accuracy\n",
    "#             pass\n",
    "\n",
    "#         # Get and print validation loss and accuracy\n",
    "        \n",
    "\n",
    "#         # Print a summary\n",
    "#         pass\n",
    "\n",
    "#     # Retrieves and returns parameters of trainable layers\n",
    "#     def get_parameters(self):\n",
    "\n",
    "#         # Create a list for parameters\n",
    "        \n",
    "\n",
    "#         # Iterable trainable layers and get their parameters\n",
    "#         for layer in self.trainable_layers:\n",
    "#             pass\n",
    "\n",
    "#         # Return a list\n",
    "#         pass\n",
    "\n",
    "#     # Updates the model with new parameters\n",
    "#     def set_parameters(self, parameters):\n",
    "\n",
    "#         # Iterate over the parameters and layers and update each layers with each set of the parameters\n",
    "#         for parameter_set, layer in zip(parameters, self.trainable_layers):\n",
    "#             layer.set_parameters(*parameter_set)\n",
    "\n",
    "#     # Predicts on the samples\n",
    "\n",
    "#     def predict(self, X, *, batch_size=None):\n",
    "\n",
    "#         # Default value if batch size is not being set\n",
    "        \n",
    "\n",
    "#         # Calculate number of steps\n",
    "#         if batch_size is not None:\n",
    "            \n",
    "#             # calculate prediction steps according to the batch size\n",
    "            \n",
    "\n",
    "#             # Dividing rounds down. If there are some remaining data, but not a full batch, this won't include it\n",
    "#             # Add `1` to include this not full batch\n",
    "#             if prediction_steps * batch_size < len(X):\n",
    "#                 pass\n",
    "\n",
    "#         # Model outputs - create empty array\n",
    "        \n",
    "\n",
    "#         # Iterate over steps\n",
    "#         for step in range(prediction_steps):\n",
    "\n",
    "#             # If batch size is not set - train using one step and full dataset\n",
    "#             if batch_size is None:\n",
    "#                 pass\n",
    "\n",
    "#             # Otherwise slice a batch\n",
    "#             else:\n",
    "#                 batch_X = X[step*batch_size:(step+1)*batch_size]\n",
    "\n",
    "#             # Perform the forward pass\n",
    "#             pass\n",
    "\n",
    "#             # Append batch prediction to the list of predictions\n",
    "            \n",
    "\n",
    "#         # Stack and return results\n",
    "#         return np.vstack(output)\n",
    "\n",
    "# # MNIST dataset (train + test)\n",
    "\n",
    "# def download_minist_dataset():\n",
    "#     # downloads and unzips the mnist dataset to the computer\n",
    "    \n",
    "#     import os\n",
    "#     import urllib.request\n",
    "#     import urllib \n",
    "#     from zipfile import ZipFile\n",
    "#     URL = 'https://nnfs.io/datasets/fashion_mnist_images.zip'\n",
    "#     FILE = 'fashion_mnist_images.zip'\n",
    "#     FOLDER = 'fashion_mnist_images'\n",
    "#     if not os.path.isfile(FILE):\n",
    "#         print(f'Downloading {URL} and saving as {FILE}...')\n",
    "#         urllib.request.urlretrieve(URL, FILE)\n",
    "#     print('Unzipping images...')\n",
    "#     with ZipFile(FILE) as zip_images:\n",
    "#         zip_images.extractall(FOLDER)\n",
    "#     print('Done!')\n",
    "\n",
    "# def load_mnist_dataset(dataset, path):\n",
    "#     # loads and preprocesses the mnist dataset\n",
    "\n",
    "#     import cv2 # to install: pip install opencv-python\n",
    "#     import os\n",
    "#     # Scan all the directories and create a list of labels\n",
    "#     labels = os.listdir(os.path.join(path, dataset))\n",
    "#     # Create lists for samples and labels\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     # For each label folder\n",
    "#     for label in labels:\n",
    "#         # And for each image in given folder\n",
    "#         for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "#             # Read the image\n",
    "#             image = cv2.imread(os.path.join(\n",
    "#                 path, dataset, label, file), cv2.IMREAD_UNCHANGED)\n",
    "#             # And append it and a label to the lists\n",
    "#             X.append(image)\n",
    "#             y.append(label)\n",
    "#     # Convert the data to proper numpy arrays and return\n",
    "#     return np.array(X), np.array(y).astype('uint8')\n",
    "\n",
    "# def create_data_mnist(path):\n",
    "#     # Load both sets separately\n",
    "#     X, y = load_mnist_dataset('train', path)\n",
    "#     X_test, y_test = load_mnist_dataset('test', path)\n",
    "#     # And return all the data\n",
    "#     return X, y, X_test, y_test\n",
    "\n",
    "# print(\"we here 1\")\n",
    "# ## Main code to run everything\n",
    "\n",
    "# # Create dataset\n",
    "# download_minist_dataset()\n",
    "# X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')\n",
    "\n",
    "# # Shuffle the training dataset\n",
    "# pass\n",
    "\n",
    "# # Scale and reshape samples\n",
    "# pass\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = Model()\n",
    "# # Add layers\n",
    "\n",
    "# # make sure you have the right output size that matches the number of classes in the trainingset\n",
    "# # model.add(Layer_Dense(X.shape[1],))\n",
    "# # model.add(Activation_ReLU())\n",
    "# # model.add(Layer_Dense())\n",
    "# # model.add(Activation_ReLU())\n",
    "# # model.add(Layer_Dense(, 10))\n",
    "# # model.add(Activation_Softmax())\n",
    "\n",
    "# # Set loss, optimizer and accuracy objects - remember to change the values to your needs\n",
    "# model.set(loss=Loss_CategoricalCrossentropy(),\n",
    "#           optimizer=Optimizer_SGD(\n",
    "#               learning_rate=0,\n",
    "#               decay=0,\n",
    "#               momentum=0),\n",
    "#           accuracy=Accuracy_Categorical())\n",
    "# print(\"we here 2\")\n",
    "# # Finalize the model\n",
    "# # model.finalize()\n",
    "# # Train the model\n",
    "# print(\"we here 3\")\n",
    "# model.train(X,\n",
    "#             y,\n",
    "#             validation_data=(X_test,\n",
    "#                              y_test),\n",
    "#             epochs=0,\n",
    "#             batch_size=0,\n",
    "#             print_every=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Rectified Linear Activation Function\n",
    "# class Activation_ReLU:\n",
    "#     # Forward pass\n",
    "#     def forward(self, inputs):\n",
    "#         self.output = np.maximum(0, inputs) \n",
    "#         # if input > 0 then output is input otherwise output is 0 \n",
    "        \n",
    "# Softmax activation\n",
    "# # In softmax activation we will take the normalized exponential values of the values given\n",
    "# class Activation_Softmax:\n",
    "#     # Forward pass\n",
    "#     def forward(self, inputs):\n",
    "#         # Get unnormalized probabilities\n",
    "#         exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "#         # we are substracting the max value to prevent the overflow error\n",
    "#         # axis = 1 makes the operation rowwise\n",
    "#         # keepdims is Keep dimensions so the output will have same dimension as input\n",
    "        \n",
    "#         # Normalize them for each sample\n",
    "#         probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "#         self.output = probabilities\n",
    "\n",
    "# MNIST dataset (train + test)\n",
    "\n",
    "def download_minist_dataset():\n",
    "    # downloads and unzips the mnist dataset to the computer    \n",
    "    import os\n",
    "    import urllib.request\n",
    "    import urllib \n",
    "    from zipfile import ZipFile\n",
    "    URL = 'https://nnfs.io/datasets/fashion_mnist_images.zip'\n",
    "    FILE = 'fashion_mnist_images.zip'\n",
    "    FOLDER = 'fashion_mnist_images'\n",
    "    if not os.path.isfile(FILE):\n",
    "        print(f'Downloading {URL} and saving as {FILE}...')\n",
    "        urllib.request.urlretrieve(URL, FILE)\n",
    "    print('Unzipping images...')\n",
    "    with ZipFile(FILE) as zip_images:\n",
    "        zip_images.extractall(FOLDER)\n",
    "    print('Done!')\n",
    "\n",
    "def load_mnist_dataset(dataset, path):\n",
    "    # loads and preprocesses the mnist dataset\n",
    "    import cv2 # to install: pip install opencv-python\n",
    "    import os\n",
    "    # Scan all the directories and create a list of labels\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "    # Create lists for samples and labels\n",
    "    X = []\n",
    "    y = []\n",
    "    counter = 0\n",
    "    # For each label folder\n",
    "    for label in labels:\n",
    "        \n",
    "        \n",
    "        # And for each image in given folder\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            counter += 1\n",
    "            if dataset == 'train' and counter >= 20_000:\n",
    "                break\n",
    "            if dataset == 'test' and counter >= 4_000:\n",
    "                break\n",
    "                        # Read the image\n",
    "            image = cv2.imread(os.path.join(\n",
    "                path, dataset, label, file), cv2.IMREAD_UNCHANGED)\n",
    "            # And append it and a label to the lists\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "    # Convert the data to proper numpy arrays and return\n",
    "    print(counter)\n",
    "    return np.array(X), np.array(y).astype('uint8')\n",
    "\n",
    "def create_data_mnist(path):\n",
    "    # Load both sets separately\n",
    "    X, y = load_mnist_dataset('train', path)\n",
    "    X_test, y_test = load_mnist_dataset('test', path)\n",
    "    # And return all the data\n",
    "    return X, y, X_test, y_test\n",
    "\n",
    "\n",
    "## Main code to run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.001, loss: 2.303, lr: 0.25\n",
      "epoch: 100, acc: 0.781, loss: 0.884, lr: 0.25\n",
      "epoch: 200, acc: 0.852, loss: 0.419, lr: 0.25\n",
      "epoch: 300, acc: 0.890, loss: 0.333, lr: 0.25\n",
      "epoch: 400, acc: 0.908, loss: 0.290, lr: 0.25\n",
      "epoch: 500, acc: 0.917, loss: 0.262, lr: 0.25\n",
      "epoch: 600, acc: 0.922, loss: 0.243, lr: 0.25\n",
      "epoch: 700, acc: 0.926, loss: 0.230, lr: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Scale and reshape samples\n",
    "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) -\n",
    "             127.5) / 127.5\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(784, 64) # input layer\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU() \n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense3 = Layer_Dense(64, 10)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate=0.25)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(701):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    activation2.forward(dense2.output)\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense3.forward(activation2.output)\n",
    "\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense3.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 10:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "#     print(loss_activation.dinputs.T.shape)\n",
    "    dense3.backward(loss_activation.dinputs)\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8877219304826206\n"
     ]
    }
   ],
   "source": [
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "activation2.forward(dense2.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense3.forward(activation2.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense3.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 10:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
